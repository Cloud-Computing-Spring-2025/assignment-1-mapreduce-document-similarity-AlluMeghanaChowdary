[![Open in Visual Studio Code](https://classroom.github.com/assets/open-in-vscode-2e0aaae1b6195c2367325f4f02e2d04e9abb55f0b24a779b69b11b9e10269abc.svg)](https://classroom.github.com/online_ide?assignment_repo_id=18028173&assignment_repo_type=AssignmentRepo)
### **ðŸ“Œ Document Similarity Using Hadoop MapReduce**  

#### **Objective**  
The goal of this assignment is to compute the **Jaccard Similarity** between pairs of documents using **MapReduce in Hadoop**. You will implement a MapReduce job that:  
1. Extracts words from multiple text documents.  
2. Identifies which words appear in multiple documents.  
3. Computes the **Jaccard Similarity** between document pairs.  
4. Outputs document pairs with similarity **above 50%**.  

# Approach and Implementation 
This project calculates the similarity between different text documents using Hadoop MapReduce. We use a method called Jaccard Similarity to compare the documents based on the words they contain. The formula is:
JaccardÂ SimilarityÂ (A,Â B)= âˆ£Aâˆ©Bâˆ£ / âˆ£AâˆªBâˆ£ 
The Mapper reads each document and breaks it into words. It then sends each document and its words as pairs to the next step. For example, it might send (Document1, "word1") and (Document1, "word2").
The Reducer takes all the words for each document and compares them with the words from other documents. It then calculates the Jaccard Similarity between pairs of documents and sends the result.

# Challenges faced
â€‹I started by practicing with simple MapReduce tasks and gradually worked my way up to solving this problem. I also tested it with small files before trying larger ones.
I split the input files into smaller parts and processed them in steps. I also adjusted the memory settings in Hadoop to handle bigger files.

This project helped me understand how to use Hadoop MapReduce to solve real-world problems. I learned how to break a big task into smaller steps, process large datasets, and use the output to calculate something useful, like document similarity.
---

### **ðŸ“¥ Example Input**  

You will be given multiple text documents. Each document will contain several words. Your task is to compute the **Jaccard Similarity** between all pairs of documents based on the set of words they contain.  

#### **Example Documents**  

##### **doc1.txt**  
```
hadoop is a distributed system
```

##### **doc2.txt**  
```
hadoop is used for big data processing
```

##### **doc3.txt**  
```
big data is important for analysis
```

---

# ðŸ“ Jaccard Similarity Calculator

## Overview

The Jaccard Similarity is a statistic used to gauge the similarity and diversity of sample sets. It is defined as the size of the intersection divided by the size of the union of two sets.

## Formula

The Jaccard Similarity between two sets A and B is calculated as:

```
Jaccard Similarity = |A âˆ© B| / |A âˆª B|
```

Where:
- `|A âˆ© B|` is the number of words common to both documents
- `|A âˆª B|` is the total number of unique words in both documents

## Example Calculation

Consider two documents:
 
**doc1.txt words**: `{hadoop, is, a, distributed, system}`
**doc2.txt words**: `{hadoop, is, used, for, big, data, processing}`

- Common words: `{hadoop, is}`
- Total unique words: `{hadoop, is, a, distributed, system, used, for, big, data, processing}`

Jaccard Similarity calculation:
```
|A âˆ© B| = 2 (common words)
|A âˆª B| = 10 (total unique words)

Jaccard Similarity = 2/10 = 0.2 or 20%
```

## Use Cases

Jaccard Similarity is commonly used in:
- Document similarity detection
- Plagiarism checking
- Recommendation systems
- Clustering algorithms

## Implementation Notes

When computing similarity for multiple documents:
- Compare each document pair
- Output pairs with similarity > 50%

### **ðŸ“¤ Expected Output**  

The output should show the Jaccard Similarity between document pairs in the following format:  
```
(doc1, doc2) -> 60%  
(doc2, doc3) -> 50%  
```

---
# step-by-step instructions

### **ðŸ›  Environment Setup: Running Hadoop in Docker**  

Since we are using **Docker Compose** to run a Hadoop cluster, follow these steps to set up your environment.  

#### **Step 1: Install Docker & Docker Compose**  
- **Windows**: Install **Docker Desktop** and enable WSL 2 backend.  
- **macOS/Linux**: Install Docker using the official guide: [Docker Installation](https://docs.docker.com/get-docker/)  

#### **Step 2: Start the Hadoop Cluster**  
Navigate to the project directory where `docker-compose.yml` is located and run:  
```sh
docker-compose up -d
```  
This will start the Hadoop NameNode, DataNode, and ResourceManager services.  

---

### **ðŸ“¦ Building and Running the MapReduce Job with Maven**  

#### **Step 1: Build the JAR File**  
Ensure Maven is installed, then navigate to your project folder and run:  
```sh
mvn clean package
```  
This will generate a JAR file inside the `target` directory.  

#### **Step 2: Move JAR File to Shared Folder
Move the generated JAR file to a shared folder for easy access:
```sh
mv target/*.jar shared-folder/code/
```

#### **Step 3: Copy the JAR File to the Hadoop Container**  
Move the compiled JAR into the running Hadoop container:  
```sh
docker cp input/shared-folder/code/DocumentSimilarity-0.0.1-SNAPSHOT.jar resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/
```

#### **Step 4: Copy the Dataset to Docker Container**  
Copy the dataset to the Hadoop ResourceManager container:
```sh
docker cp input/input.txt resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/
```

---

### ** Connect to Docker Container**  

#### **Step 1: Connect to Docker Container**  
Access the Hadoop ResourceManager container:  
```sh
docker exec -it resourcemanager /bin/bash
```

#### **Step 2: Navigate to the Hadoop directory:**  
Copy your local dataset into the Hadoop clusterâ€™s HDFS:  
```sh
cd /opt/hadoop-3.2.1/share/hadoop/mapreduce/
```

---

### **ðŸ“‚ Uploading Data to HDFS**  

#### **Step 1: Create an Input Directory in HDFS**  
Inside the Hadoop container, create the directory where input files will be stored:  
```sh
hadoop fs -mkdir -p /input/dataset
```

#### **Step 2: Upload Dataset to HDFS**  
Copy your local dataset into the Hadoop clusterâ€™s HDFS:  
```sh
hadoop fs -put ./input.txt /input/dataset
```

---

### **ðŸš€ Running the MapReduce Job**  

Run the Hadoop job using the JAR file inside the container:  
```sh
hadoop jar /opt/hadoop-3.2.1/share/hadoop/mapreduce/DocumentSimilarity-0.0.1-SNAPSHOT.jar com.example.controller.DocumentSimilarityDriver /input/dataset/input.txt /output
```

---

### **ðŸ“Š Retrieving the Output**  

To view the results stored in HDFS:  
```sh
hdfs dfs -cat /output_final/part-r-00000
```

If you want to download the output to your local machine:  
```sh
docker cp resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/output/ shared-folder/output/
```
---
